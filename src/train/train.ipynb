{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import resource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Conv1DTranspose, Concatenate,\n",
    "    Add, Activation, Multiply, Dropout, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    ")\n",
    "\n",
    "\n",
    "def load_data(dataset_path, duration=30):\n",
    "    tracks = []\n",
    "    required_files = ['mixed_original.mp3', 'mixed_with_noise.mp3', 'mixed_with_pitch_shift.mp3', 'mixed_with_speed_change.mp3', 'fiddle.mp3', 'flute.mp3', 'xylophone.mp3']\n",
    "\n",
    "    for folder in os.listdir(dataset_path):\n",
    "      for mixed_file in ['mixed_original.mp3', 'mixed_with_noise.mp3', 'mixed_with_pitch_shift.mp3', 'mixed_with_speed_change.mp3']:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            files_in_folder = os.listdir(folder_path)\n",
    "            if all(file in files_in_folder for file in required_files):\n",
    "                mixture_path = os.path.join(folder_path, mixed_file)\n",
    "                fiddle_path = os.path.join(folder_path, 'fiddle.mp3')\n",
    "                flute_path = os.path.join(folder_path, 'flute.mp3')\n",
    "                xylophone_path = os.path.join(folder_path, 'xylophone.mp3')\n",
    "\n",
    "                mixture, sr = librosa.load(mixture_path, sr=None, duration=duration)\n",
    "                fiddle, _ = librosa.load(fiddle_path, sr=sr, duration=duration)\n",
    "                flute, _ = librosa.load(flute_path, sr=sr, duration=duration)\n",
    "                xylophone, _ = librosa.load(xylophone_path, sr=sr, duration=duration)\n",
    "\n",
    "                if len(mixture) == duration * sr and len(fiddle) == duration * sr and len(flute) == duration * sr and len(xylophone) == duration * sr:\n",
    "                    mixture = mixture.reshape(-1, 1)\n",
    "                    instruments = np.stack([fiddle, flute, xylophone], axis=-1)\n",
    "                    tracks.append((mixture, instruments))\n",
    "    return tracks, sr\n",
    "\n",
    "dataset_path = '/content/DSTHAI/content/DSTHAI'\n",
    "tracks, sample_rate = load_data(dataset_path)\n",
    "\n",
    "\n",
    "def split_dataset(tracks, split_ratio=0.8):\n",
    "    train_tracks, val_tracks = train_test_split(tracks, train_size=split_ratio, random_state=42)\n",
    "    return train_tracks, val_tracks\n",
    "train_tracks, val_tracks = split_dataset(tracks, split_ratio=0.8)\n",
    "\n",
    "def data_generator(tracks, batch_size):\n",
    "    while True:\n",
    "        for start in range(0, len(tracks), batch_size):\n",
    "            end = min(start + batch_size, len(tracks))\n",
    "            batch_tracks = tracks[start:end]\n",
    "\n",
    "            mixtures = np.array([track[0] for track in batch_tracks])\n",
    "            instruments = np.array([track[1] for track in batch_tracks])\n",
    "\n",
    "            yield mixtures, instruments\n",
    "\n",
    "batch_size = 12\n",
    "train_generator = data_generator(train_tracks, batch_size)\n",
    "val_generator = data_generator(val_tracks, batch_size)\n",
    "\n",
    "def create_wave_unet_model(input_shape):\n",
    "    def downsampling_block(x, filters, kernel_size, pool_size):\n",
    "        conv = Conv1D(filters, kernel_size, activation='relu', padding='same')(x)\n",
    "        downsampled = MaxPooling1D(pool_size)(conv)\n",
    "        return downsampled, conv\n",
    "\n",
    "    def upsampling_block(x, skip_connection, filters, kernel_size, upsample_size):\n",
    "        upsampled = Conv1DTranspose(filters, kernel_size, strides=upsample_size, activation='relu', padding='same', kernel_initializer=he_normal())(x)\n",
    "        concat = Concatenate()([upsampled, skip_connection])\n",
    "        return concat\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    skip_connections = []\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling blocks\n",
    "    for filters in [64, 128, 256]:\n",
    "        x, skip = downsampling_block(x, filters, 15, 2)\n",
    "        skip_connections.append(skip)\n",
    "\n",
    "    # Bottleneck\n",
    "    x = Conv1D(512, 15, activation='relu', padding='same')(x)\n",
    "\n",
    "    # Upsampling blocks\n",
    "    for filters, skip in zip([256, 128, 64], reversed(skip_connections)):\n",
    "        x = upsampling_block(x, skip, filters, 5, 2)\n",
    "\n",
    "    outputs = Conv1D(3, 1, activation='linear', padding='same')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (None, 1)\n",
    "model = create_wave_unet_model(input_shape)\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.004, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "model.compile(optimizer=adam_optimizer, loss='mse', metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "model.summary()\n",
    "\n",
    "class keepDataCSV(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.csv_file = open(os.path.join(save_path, 'training_stats.csv'), 'w')\n",
    "        self.csv_writer = csv.writer(self.csv_file)\n",
    "        self.csv_writer.writerow(['Epoch Number', 'Training Loss', 'Validate Loss', 'MAE', 'Validate MAE', 'Learning Rate', 'Elapsed Time', 'RAM', 'CPU'])\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Record the start time when training begins\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Calculate RAM and CPU usage\n",
    "        ram_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024 ** 2\n",
    "        cpu_usage = resource.getrusage(resource.RUSAGE_SELF).ru_utime + resource.getrusage(resource.RUSAGE_SELF).ru_stime\n",
    "\n",
    "        # Write epoch data to CSV\n",
    "        self.csv_writer.writerow([epoch + 1, logs.get('loss'), logs.get('val_loss'), logs.get('mean_absolute_error'), logs.get('val_mean_absolute_error'),\n",
    "                                  logs.get('lr'), time.time() - self.start_time, ram_usage, cpu_usage])\n",
    "        self.csv_file.flush()\n",
    "\n",
    "# Create the callback object\n",
    "save_path = '/content/ModelComplete'\n",
    "keepDataCSV = keepDataCSV(save_path)\n",
    "# Callbacks\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('/content/ModelComplete/model_{epoch:02d}.h5', save_freq='epoch', period=5)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=3, min_lr=1e-6)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "epochs = 20 # Adjust as needed\n",
    "steps_per_epoch = len(tracks) // batch_size\n",
    "validation_steps = len(val_tracks) // batch_size\n",
    "\n",
    "model.fit(train_generator, epochs=epochs, callbacks=[early_stopping, lr_scheduler, checkpoint, keepDataCSV],\n",
    "          steps_per_epoch=steps_per_epoch, validation_data=val_generator, validation_steps=validation_steps)\n",
    "\n",
    "keepDataCSV.csv_file.close()\n",
    "\n",
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
